{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import codecs\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read documents from collection\n",
    "The documents in the Reuters collection are in SGML format. So first, we need to be able to read SGML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TT:\n",
    "    \"\"\"Tag Tuple index names\"\"\"\n",
    "    TAG = 0\n",
    "    META = 1\n",
    "    SUBTAGS = 2\n",
    "    CONTENT = 3\n",
    "\n",
    "def ReadDocumentsFromFile(path, preview=0):\n",
    "    global parts\n",
    "    global partsIndex\n",
    "\n",
    "    def ParseSGML(returnTag=None, allowDuplicates=True, root=True):\n",
    "        \"\"\"Parses *parts* until returnTag or the end of *parts* reached; returns parsed content as a map\"\"\"\n",
    "        global parts\n",
    "        global partsIndex\n",
    "\n",
    "        # Force allowDuplicates \"True\" since \"False\" isn't fully implemented\n",
    "        allowDuplicates = True\n",
    "\n",
    "        subTags = dict()\n",
    "        content = \"\"\n",
    "\n",
    "        while partsIndex < numParts:\n",
    "            part = parts[partsIndex]\n",
    "            partsIndex += 1\n",
    "\n",
    "            # Remove excess whitespace\n",
    "            part = \" \".join(part.split())\n",
    "            if len(part) == 0:\n",
    "                continue\n",
    "\n",
    "            # Process tags\n",
    "            if part[0] == \"<\":\n",
    "                assert(part[-1] == \">\")\n",
    "\n",
    "                # Ignore comments\n",
    "                if part[1] == \"!\":\n",
    "                    continue\n",
    "\n",
    "                # Return contents when *returnTag* is reached\n",
    "                if part[1] == \"/\":\n",
    "                    tag = part[2:-1]\n",
    "                    assert(tag == returnTag)\n",
    "                    return (subTags, content)\n",
    "\n",
    "                # Add new tags to the stack\n",
    "                tagAndMeta = part[1:-1].split()\n",
    "                tag = tagAndMeta[0]\n",
    "\n",
    "                # Store key-value pairs associated with the tag (formatted \"KEY1=VALUE1 KEY2=VALUE2 ...\")\n",
    "                tagMeta = dict([x.split(\"=\") for x in tagAndMeta[1::]])\n",
    "\n",
    "                # Collect content and tags within the current tag\n",
    "                (tagSubTags, tagContent) = ParseSGML(\n",
    "                    returnTag=tag,\n",
    "                    allowDuplicates=False,\n",
    "                    root=False\n",
    "                )\n",
    "\n",
    "                tagTuple = (\n",
    "                    tag,\n",
    "                    tagMeta,\n",
    "                    tagSubTags,\n",
    "                    tagContent\n",
    "                )\n",
    "\n",
    "                if allowDuplicates:\n",
    "                    if tag not in subTags:\n",
    "                        subTags[tag] = list()\n",
    "                    subTags[tag].append(tagTuple)\n",
    "                else:\n",
    "                    subTags[tag] = tagTuple\n",
    "\n",
    "            # Process text\n",
    "            else:\n",
    "                content += (\" \" if len(content) > 0 else \"\") + part\n",
    "\n",
    "        return (subTags, content)\n",
    "\n",
    "\n",
    "    # Create one big string\n",
    "    document = \"\"\n",
    "    #with open(path, \"r\") as file:\n",
    "    with codecs.open(path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        for i, line in enumerate(file):\n",
    "            document += line.replace('\\n', ' ')\n",
    "\n",
    "            # Show the first several lines of the file\n",
    "            if i < preview:\n",
    "                print('%d: %s' % (i, line), end=\"\")\n",
    "\n",
    "    # Show the last line as well\n",
    "    if preview:\n",
    "        print(\"...\\n%d: %s\" % (i, line))\n",
    "    \n",
    "    parts = re.split(\"(<.*?>)\", document)\n",
    "    numParts = len(parts)\n",
    "    partsIndex = 0\n",
    "    allData = ParseSGML()\n",
    "    \n",
    "    return allData[0][\"REUTERS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look through the data directory for .sgm files and extract their contents. Each file is supposed to contain 1000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataFilePaths(directory):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    paths = [join(directory, filePath) for filePath in listdir(directory) if isfile(join(directory, filePath)) and filePath[-4:] == \".sgm\"]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <!DOCTYPE lewis SYSTEM \"lewis.dtd\">\n",
      "1: <REUTERS TOPICS=\"YES\" LEWISSPLIT=\"TRAIN\" CGISPLIT=\"TRAINING-SET\" OLDID=\"8914\" NEWID=\"4001\">\n",
      "2: <DATE>11-MAR-1987 18:04:17.59</DATE>\n",
      "3: <TOPICS></TOPICS>\n",
      "4: <PLACES><D>canada</D></PLACES>\n",
      "5: <PEOPLE></PEOPLE>\n",
      "6: <ORGS></ORGS>\n",
      "7: <EXCHANGES></EXCHANGES>\n",
      "8: <COMPANIES></COMPANIES>\n",
      "9: <UNKNOWN> \n",
      "10: &#5;&#5;&#5;M\n",
      "11: &#22;&#22;&#1;f0849&#31;reute\n",
      "12: d f BC-INCO-SEES-NO-MAJOR-IM   03-11 0133</UNKNOWN>\n",
      "13: <TEXT>&#2;\n",
      "14: <TITLE>INCO SEES NO MAJOR IMPACT FROM DOW REMOVAL</TITLE>\n",
      "15: <DATELINE>    TORONTO, March 11 - </DATELINE><BODY>Inco Ltd said it did not expect its\n",
      "16: earlier reported removal from the Dow Jones industrial index to\n",
      "17: make a major impact on the company's stock.\n",
      "18:     \"We don't think that individuals or institutions buy our\n",
      "19: shares because we were one of the Dow Jones industrials,\"\n",
      "...\n",
      "32587: </REUTERS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgmlDocuments = []\n",
    "previewLines = 20\n",
    "\n",
    "filePaths = GetDataFilePaths(\"reuters21578/\")\n",
    "for filePath in filePaths:\n",
    "    sgmlDocuments += ReadDocumentsFromFile(filePath, previewLines)\n",
    "    previewLines = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 21,578 documents from 22 files\n"
     ]
    }
   ],
   "source": [
    "numDocuments = len(sgmlDocuments)\n",
    "print(\"Extracted %s documents from %d files\" % (\"{:,}\".format(numDocuments), len(filePaths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an index\n",
    "P115: Scoring algorithm\n",
    "  1. \"store N/df_t  at the head of the postings for t\" to compute idf_t\n",
    "  1. \"store the term frequency tf_t,d for each postings entry\"\n",
    "  1. Use a heap as a priority queue for document scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "\n",
    "def ContentTokens(content):\n",
    "    # Replace symbols and numbers with spaces\n",
    "    content = \"\".join([(c if c.isalpha() else \" \") for c in content if c != \"'\"])\n",
    "    allTokens = content.split()\n",
    "    return [x.lower() for x in allTokens if len(x) > 1 and x.replace(\"'\", \"a\").isalpha()]\n",
    "\n",
    "def CombinedTermCounts(a, b):\n",
    "    for term, count in b.items():\n",
    "        a[term] = (a[term] if term in a else 0) + count\n",
    "    return a\n",
    "\n",
    "def GetTermCounts(tagTuple):\n",
    "    # Get term counts for this tag's content\n",
    "    tokens = ContentTokens(tagTuple[TT.CONTENT])\n",
    "    terms = set(tokens)\n",
    "    termCounts = dict([(x, tokens.count(x)) for x in terms])\n",
    "\n",
    "    # Accumulate term counts from sub tags\n",
    "    for _, subTagTuples in tagTuple[TT.SUBTAGS].items():\n",
    "        for subTagTuple in subTagTuples:\n",
    "            subTagTermCounts = GetTermCounts(subTagTuple)\n",
    "            termCounts = CombinedTermCounts(termCounts, subTagTermCounts)\n",
    "\n",
    "    return termCounts\n",
    "\n",
    "class TermPostings:\n",
    "    \"\"\"Postings list with metrics for a single term\"\"\"\n",
    "    def __init__(self):\n",
    "        self.postingsList = dict()\n",
    "        self.cf = 0\n",
    "        self.df = 0\n",
    "        self.idf = 0\n",
    "        self.log_idf = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each term, count occurrences in all documents\n",
    "postings = dict()\n",
    "documentTermCounts = [dict()] * numDocuments\n",
    "for i, doc in enumerate(sgmlDocuments):\n",
    "    termCounts = GetTermCounts(doc)\n",
    "    documentTermCounts[i] = termCounts\n",
    "\n",
    "    for term, count in termCounts.items():\n",
    "        if term not in postings:\n",
    "            postings[term] = TermPostings()\n",
    "        postings[term].postingsList[i] = (count)\n",
    "\n",
    "# Calculate frequency metrics\n",
    "for _, termPostings in postings.items():\n",
    "    termPostings.cf = np.sum([x for x in termPostings.postingsList.values()])\n",
    "    termPostings.idf = numDocuments / len(termPostings.postingsList)\n",
    "    termPostings.log_idf = np.log(termPostings.idf)\n",
    "\n",
    "# Sort by term\n",
    "index = OrderedDict([(x, postings[x]) for x in sorted(postings)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build document vectors\n",
    "To make vector scoring possible, we create a normalized vector representation for each document. Indexing and general performance improvements for document vector usage are not covered until chapter 7, so for now we're using brute force methods.\n",
    "\n",
    "Document vectors can be created by normalizing the document term count vectors -- we carry over the (term, count) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsVector:\n",
    "    \"\"\"Vector representation of a document or query\"\"\"\n",
    "    def __init__(self, termWeights):\n",
    "        self.termWeights = termWeights\n",
    "        self.length = 1\n",
    "\n",
    "    def CosineSimilarity(self, vector):\n",
    "        # It's more efficient if the shorter vector does the calculations\n",
    "        if len(self.termWeights) > len(vector.termWeights):\n",
    "            return vector.CosineSimilarity(self)\n",
    "\n",
    "        # Dot product\n",
    "        dot = np.sum([self.termWeights[x] * vector.termWeights[x] for x in self.termWeights if x in vector.termWeights])\n",
    "        return dot / (self.length * vector.length)\n",
    "\n",
    "    @staticmethod\n",
    "    def FromString(string):\n",
    "        # Extract tokens and count terms\n",
    "        tokens = ContentTokens(string)\n",
    "        terms = set(tokens)\n",
    "        termCounts = dict([(x, tokens.count(x)) for x in terms])\n",
    "        return WeightsVector.FromTermCounts(termCounts)        \n",
    "\n",
    "    @staticmethod\n",
    "    def FromTermCounts(termCounts):\n",
    "        # Normalize term counts\n",
    "        vectorLength = np.sqrt(np.sum([np.square(x) for x in termCounts.values()]))\n",
    "        termWeights = dict([\n",
    "            (termCount[0], termCount[1] / vectorLength)\n",
    "            for termCount in termCounts.items()\n",
    "        ])\n",
    "        return WeightsVector(termWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentVectors = [None] * numDocuments\n",
    "for i, counts in enumerate(documentTermCounts):\n",
    "    documentVectors[i] = WeightsVector.FromTermCounts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some queries using vector space scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSimilarVectors(weightsVector):\n",
    "    # Score each document vector\n",
    "    scores = dict()\n",
    "    for i, documentVector in enumerate(documentVectors):\n",
    "        scores[i] = weightsVector.CosineSimilarity(documentVector)\n",
    "    \n",
    "    # Sort by highest score\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def GetQueryVector(query):\n",
    "    return WeightsVector.FromString(query)\n",
    "\n",
    "def GetQueryScores(query):\n",
    "    queryVector = GetQueryVector(query)\n",
    "    return GetSimilarVectors(queryVector)\n",
    "\n",
    "def GetBestResult(query):\n",
    "    bestMatch = GetQueryScores(query)[0]\n",
    "    if (bestMatch[1] > 0):\n",
    "        return (bestMatch[0], bestMatch[1])\n",
    "    else:\n",
    "        print(\"No documents contain any of the queried terms \" + str(ContentTokens(query)))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('REUTERS',\n",
       " {'CGISPLIT': '\"TRAINING-SET\"',\n",
       "  'LEWISSPLIT': '\"TRAIN\"',\n",
       "  'NEWID': '\"14468\"',\n",
       "  'OLDID': '\"3451\"',\n",
       "  'TOPICS': '\"BYPASS\"'},\n",
       " {'COMPANIES': [('COMPANIES', {}, {}, '')],\n",
       "  'DATE': [('DATE', {}, {}, '7-APR-1987 15:19:48.23')],\n",
       "  'EXCHANGES': [('EXCHANGES', {}, {}, '')],\n",
       "  'ORGS': [('ORGS', {}, {}, '')],\n",
       "  'PEOPLE': [('PEOPLE', {}, {}, '')],\n",
       "  'PLACES': [('PLACES', {}, {}, '')],\n",
       "  'TEXT': [('TEXT',\n",
       "    {'TYPE': '\"UNPROC\"'},\n",
       "    {},\n",
       "    \"&#2; U.S. COCOA MERCHANT SPOT PRICES - APRIL 7 In U.S. dlrs per tonne for minimum truckload quantities ex-dock or ex-warehouse U.S. Eastern seaboard north of Hatteras seller's option: Main Crop Ivory Coast cocoa beans 2,148N Superior Bahia cocoa beans 2,062N Sanchez FAQ cocoa beans 1,945N Superior seasons Arriba cocoa beans 1,965N Ecuadorean cocoa liquor 2,541N Brazilian cocoa liquor 2,612N Pure pressed cocoa butter/African 4,646N PURE PRESSED COCOA BUTTER/OTHER 4,620N Cocoa press cake - natural 10/12 pct cocoa butter contents 975N All prices are for regular commercial quality. Reuter &#3;\")],\n",
       "  'TOPICS': [('TOPICS', {}, {}, '')],\n",
       "  'UNKNOWN': [('UNKNOWN',\n",
       "    {},\n",
       "    {},\n",
       "    '&#5;&#5;&#5;C T &#22;&#22;&#1;f2208&#31;reute u f BC-cocoa-mchts-assn-spot 04-07 0115')]},\n",
       " '')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(docID, score) = GetBestResult(\"brazilian cocoa\")\n",
    "print(\"Score: %.2f\" % score)\n",
    "sgmlDocuments[docID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.55\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('REUTERS',\n",
       " {'CGISPLIT': '\"TRAINING-SET\"',\n",
       "  'LEWISSPLIT': '\"TRAIN\"',\n",
       "  'NEWID': '\"2105\"',\n",
       "  'OLDID': '\"18523\"',\n",
       "  'TOPICS': '\"NO\"'},\n",
       " {'COMPANIES': [('COMPANIES', {}, {}, '')],\n",
       "  'DATE': [('DATE', {}, {}, '5-MAR-1987 10:48:58.69')],\n",
       "  'EXCHANGES': [('EXCHANGES', {}, {}, '')],\n",
       "  'ORGS': [('ORGS', {}, {}, '')],\n",
       "  'PEOPLE': [('PEOPLE', {}, {}, '')],\n",
       "  'PLACES': [('PLACES', {}, {}, '')],\n",
       "  'TEXT': [('TEXT',\n",
       "    {'TYPE': '\"BRIEF\"'},\n",
       "    {'TITLE': [('TITLE',\n",
       "       {},\n",
       "       {},\n",
       "       'KIDDER PEABODY ANALYST RAISES ESTIMATES, RECOMMENDATION ON PEPSICO')]},\n",
       "    '&#2; ****** Blah blah blah. &#3;')],\n",
       "  'TOPICS': [('TOPICS', {}, {}, '')],\n",
       "  'UNKNOWN': [('UNKNOWN',\n",
       "    {},\n",
       "    {},\n",
       "    '&#5;&#5;&#5;F &#22;&#22;&#1;f0408&#31;reute b f BC-******KIDDER-PEABODY 03-05 0012')]},\n",
       " '')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(docID, score) = GetBestResult(\"Kidder Peabody\")\n",
    "print(\"Score: %.2f\" % score)\n",
    "sgmlDocuments[docID]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's look at titles of the articles that are matching our queries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDocumentMetadata(document, tag):\n",
    "    for _, subTagTuples in document[TT.SUBTAGS].items():\n",
    "        for subTagTuple in subTagTuples:\n",
    "            if subTagTuple[TT.TAG] == tag and len(subTagTuple[TT.CONTENT]) > 0:\n",
    "                return subTagTuple[TT.CONTENT]\n",
    "            metadata = GetDocumentMetadata(subTagTuple, tag)\n",
    "            if len(metadata) > 0:\n",
    "                return metadata\n",
    "    return \"\"\n",
    "\n",
    "def PrintTopMatches(matches, count, meta='TITLE'):\n",
    "    print(\"DocID\\tScore\\t%s\\n\" % meta.capitalize())\n",
    "    for match in matches[:count]:\n",
    "        docID = match[0]\n",
    "        score = match[1]\n",
    "        metadata = GetDocumentMetadata(sgmlDocuments[docID], meta) if len(meta) > 0 else \"\"\n",
    "        print(\"%d\\t%.2f\\t%s\" % (docID, score, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID\tScore\tTitle\n",
      "\n",
      "13045\t0.58\t\n",
      "3505\t0.37\tCOCOA BUFFER STOCK RULES EFFECTIVE IMMEDIATELY\n",
      "3470\t0.36\tCocoa Council agrees new buffer stock rules - delegates\n",
      "1721\t0.36\tNEW YORK COCOA FUTURES EXPECTED TO OPEN LOWER\n",
      "3490\t0.35\tCOCOA BUFFER STOCK RULES TO TAKE EFFECT IMMEDIATELY - DELEGATES\n",
      "1367\t0.32\t\n",
      "16767\t0.32\tGHANA COCOA PURCHASES 1,323 TONNES IN LATEST WEEK, CUMULATIVE 216,095 TONNES - OFFICIAL\n",
      "10652\t0.31\tBRAZIL COCOA EXPORTERS UNLIKELY TO LIMIT SALES\n",
      "3585\t0.30\tCOCOA COUNCIL MEETING ENDS AFTER AGREEING RULES\n",
      "1620\t0.30\t\n"
     ]
    }
   ],
   "source": [
    "matches = GetQueryScores(\"brazilian cocoa\")\n",
    "PrintTopMatches(matches, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID\tScore\tTitle\n",
      "\n",
      "8104\t0.55\tKIDDER PEABODY ANALYST RAISES ESTIMATES, RECOMMENDATION ON PEPSICO\n",
      "11103\t0.33\tNOMURA SAYS IT SEEKS LINK WITH KIDDER PEABODY\n",
      "18750\t0.29\tPEABODY HOLDING COMPLETES ACQUISITION\n",
      "7826\t0.28\tKIDDER UNIT SELLS CMOS, INCLUDING FLOATERS\n",
      "10954\t0.25\tKIDDER, PEABODY DEFENDS EMPLOYEE DRUG TESTING\n",
      "5019\t0.25\tTHREE TRADERS INDICTED FOR INSIDER TRADING\n",
      "10150\t0.23\tAMPLICON INITIAL OFFERING AT 12.25 DLRS A SHARE\n",
      "8115\t0.21\tPEPSICO &lt;PEP> UPGRADED BY KIDDER PEABODY\n",
      "18685\t0.20\tPROPOSED OFFERINGS RECENTLY FILED WITH THE SEC\n",
      "16164\t0.19\tJOY TECHNOLOGIES SELLS 12-YEAR DEBENTURES\n"
     ]
    }
   ],
   "source": [
    "matches = GetQueryScores(\"Kidder Peabody\")\n",
    "PrintTopMatches(matches, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID\tScore\tTitle\n",
      "\n",
      "2831\t0.26\tFORD &lt;F> MAY CAR OUTPUT UP 2.2 PCT\n",
      "9529\t0.24\tFORD LATE FEBRUARY U.S. CAR SALES UP 29.3 PCT\n",
      "2823\t0.24\tFORD MAY N. AMERICAN CAR PRODUCTION 213,790, UP 2.2 PCT FROM 209,109\n",
      "9645\t0.23\tFORD CANADA FEBRUARY CAR SALES OFF SIX PCT\n",
      "12378\t0.23\tFORD MID-MARCH CAR SALES UP 15.2 PCT\n",
      "9626\t0.23\tFORD CANADA &lt;FC> FEBRUARY CAR SALES OFF SIX PCT\n",
      "18048\t0.22\tTENNECO &lt;TGT> TO TRANSPORT GAS ON OPEN ACCESS\n",
      "833\t0.22\tLEAR PETROLEUM &lt;LPT> CONSOLIDATES GAS UNITS\n",
      "17660\t0.21\tJAPAN FIRMS TO LAUNCH SALES OF 100 OCTANE GASOLINE\n",
      "18936\t0.20\tFORD &lt;F> MARCH U.S. CAR PRODUCTION UP\n"
     ]
    }
   ],
   "source": [
    "matches = GetQueryScores(\"car automobile gasoline gas industry ford\")\n",
    "PrintTopMatches(matches, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID\tScore\tTitle\n",
      "\n",
      "1060\t0.23\tTOYOTA ANNOUNCES HIGHER DOMESTIC CAR SALES\n",
      "18048\t0.22\tTENNECO &lt;TGT> TO TRANSPORT GAS ON OPEN ACCESS\n",
      "833\t0.22\tLEAR PETROLEUM &lt;LPT> CONSOLIDATES GAS UNITS\n",
      "9670\t0.21\tTOYOTA MOTOR U.S.A. FEBRUARY CAR SALES DOWN\n",
      "17660\t0.21\tJAPAN FIRMS TO LAUNCH SALES OF 100 OCTANE GASOLINE\n",
      "6867\t0.20\tTOYOTA &lt;TOYOY> HIKES 1987 PRICES\n",
      "4241\t0.20\tTOYOTA LIKELY TO NAME AMERICAN FIRMS AS SUPPLIERS\n",
      "10321\t0.19\tU.S.SENATE LIFTS SOME BANS ON NATURAL GAS\n",
      "2360\t0.19\tMORGAN STANLEY GROUP &lt;MS> UNIT IN GAS DEAL\n",
      "16494\t0.19\tBROOKLYN UNION&lt;BU> SEEN HURT BY PIPELINE CLOSURE\n"
     ]
    }
   ],
   "source": [
    "matches = GetQueryScores(\"car automobile gasoline gas industry toyota\")\n",
    "PrintTopMatches(matches, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID\tScore\tTitle\n",
      "\n",
      "11097\t0.21\tDOW JONES INDUSTRIAL AVERAGE DOWN MORE THAN 13.2 PCT, EXCEEDS PERCENTAGE DROP IN 1929\n",
      "15994\t0.18\tASCS TAKES STEPS TO EASE SALES OF CCC SOYBEANS\n",
      "9550\t0.18\tCEREALS MCAS TO BE UNCHANGED NEXT WEEK\n",
      "13340\t0.18\tHOUSE COMMITTEE SLASHES \"STAR WARS\" FUNDING\n",
      "21439\t0.18\tCEREALS MCAS TO BE UNCHANGED NEXT WEEK\n",
      "548\t0.18\tU.S. MARKET LOAN NOT THAT ATTRACTIVE-BOSCHWITZ\n",
      "3704\t0.17\tJAPAN BUYS LARGE AMOUNT OF BRAZILIAN SOYBEANS\n",
      "20090\t0.17\tCEREALS MCAS TO BE UNCHANGED NEXT WEEK\n",
      "18810\t0.17\tPOLL FINDS MOST THINK HOSTILE TAKEOVERS HARMFUL\n",
      "9404\t0.17\tCBT TRADERS LOOK AHEAD TO SPRING PLANTINGS\n"
     ]
    }
   ],
   "source": [
    "matches = GetQueryScores(\"\\\"soybeans\\\" counts more for scoring than these other words do\")\n",
    "PrintTopMatches(matches, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also look for documents that are similar to each other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRITISH MINISTER GOES TO TOKYO ASKING FREER ACCESS'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docID = 1252 # Some random document\n",
    "document = sgmlDocuments[docID]\n",
    "GetDocumentMetadata(document, 'TITLE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocID\tScore\tTitle\n",
      "\n",
      "1252\t1.00\tBRITISH MINISTER GOES TO TOKYO ASKING FREER ACCESS\n",
      "1033\t0.82\tUK MINISTER LOOKS TO EASE TENSION ON TOKYO TRIP\n",
      "3454\t0.81\tUK MAY REVOKE JAPANESE FINANCIAL LICENSES\n",
      "18602\t0.81\tTOSHIBA COULD BE FIRST TO FEEL U.K. TRADE ANGER\n",
      "13521\t0.79\tBRITAIN, JAPAN CLASH OVER STOCK MARKET ACCESS\n",
      "1648\t0.77\tBRITAIN WARNS JAPAN OVER TRADE ROW\n",
      "19519\t0.77\tUK HOPES HOWARD'S TOKYO TRIP WILL EASE TENSIONS\n",
      "20775\t0.76\tEUROPE ON SIDELINES IN U.S-JAPAN MICROCHIP ROW\n",
      "5094\t0.76\tBRITISH MINISTER SAYS HE WARNED TOKYO OF SANCTIONS\n",
      "3622\t0.76\tNAKASONE SOUNDS CONCILIATORY NOTE IN CHIP DISPUTE\n"
     ]
    }
   ],
   "source": [
    "documentVector = documentVectors[docID]\n",
    "matches = GetSimilarVectors(documentVector)\n",
    "PrintTopMatches(matches, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
