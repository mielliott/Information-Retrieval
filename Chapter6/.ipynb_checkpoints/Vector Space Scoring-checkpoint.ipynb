{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read documents from collection\n",
    "The documents in the Reuters collection are in SGML format. So first, we need to be able to read SGML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TT:\n",
    "    \"\"\"Tag Tuple index names\"\"\"\n",
    "    TAG = 0\n",
    "    META = 1\n",
    "    SUBTAGS = 2\n",
    "    CONTENT = 3\n",
    "\n",
    "def ReadDocumentsFromFile(path, preview=0):\n",
    "    global parts\n",
    "    global partsIndex\n",
    "\n",
    "    def ParseSGML(returnTag=None, allowDuplicates=True, root=True):\n",
    "        \"\"\"Parses *parts* until returnTag or the end of *parts* reached; returns parsed content as a map\"\"\"\n",
    "        global parts\n",
    "        global partsIndex\n",
    "\n",
    "        # Force allowDuplicates \"True\" since \"False\" isn't fully implemented\n",
    "        allowDuplicates = True\n",
    "\n",
    "        subTags = dict()\n",
    "        content = \"\"\n",
    "\n",
    "        while partsIndex < numParts:\n",
    "            part = parts[partsIndex]\n",
    "            partsIndex += 1\n",
    "\n",
    "            # Remove excess whitespace\n",
    "            part = \" \".join(part.split())\n",
    "            if len(part) == 0:\n",
    "                continue\n",
    "\n",
    "            # Process tags\n",
    "            if part[0] == \"<\":\n",
    "                assert(part[-1] == \">\")\n",
    "\n",
    "                # Ignore comments\n",
    "                if part[1] == \"!\":\n",
    "                    continue\n",
    "\n",
    "                # Return contents when *returnTag* is reached\n",
    "                if part[1] == \"/\":\n",
    "                    tag = part[2:-1]\n",
    "                    assert(tag == returnTag)\n",
    "                    return (subTags, content)\n",
    "\n",
    "                # Add new tags to the stack\n",
    "                tagAndMeta = part[1:-1].split()\n",
    "                tag = tagAndMeta[0]\n",
    "\n",
    "                # Store key-value pairs associated with the tag (formatted \"KEY1=VALUE1 KEY2=VALUE2 ...\")\n",
    "                tagMeta = dict([x.split(\"=\") for x in tagAndMeta[1::]])\n",
    "\n",
    "                # Collect content and tags within the current tag\n",
    "                (tagSubTags, tagContent) = ParseSGML(\n",
    "                    returnTag=tag,\n",
    "                    allowDuplicates=False,\n",
    "                    root=False\n",
    "                )\n",
    "\n",
    "                tagTuple = (\n",
    "                    tag,\n",
    "                    tagMeta,\n",
    "                    tagSubTags,\n",
    "                    tagContent\n",
    "                )\n",
    "\n",
    "                if allowDuplicates:\n",
    "                    if tag not in subTags:\n",
    "                        subTags[tag] = list()\n",
    "                    subTags[tag].append(tagTuple)\n",
    "                else:\n",
    "                    subTags[tag] = tagTuple\n",
    "\n",
    "            # Process text\n",
    "            else:\n",
    "                content += (\" \" if len(content) > 0 else \"\") + part\n",
    "\n",
    "        return (subTags, content)\n",
    "\n",
    "\n",
    "    # Create one big string\n",
    "    document = \"\"\n",
    "    with open(path, \"r\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            document += line.replace('\\n', ' ')\n",
    "\n",
    "            # Show the first several lines of the file\n",
    "            if i < preview:\n",
    "                print('%d: %s' % (i, line), end=\"\")\n",
    "\n",
    "    # Show the last line as well\n",
    "    if preview:\n",
    "        print(\"...\\n%d: %s\" % (i, line))\n",
    "    \n",
    "    parts = re.split(\"(<.*?>)\", document)\n",
    "    numParts = len(parts)\n",
    "    partsIndex = 0\n",
    "    allData = ParseSGML()\n",
    "    \n",
    "    return allData[0][\"REUTERS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look through the data directory for .sgm files and extract their contents. Each file is supposed to contain 1000 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetDataFilePaths(directory):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    paths = [filePath for filePath in listdir(directory) if isfile(join(directory, filePath)) and filePath[-4:] == \".sgm\"]\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <!DOCTYPE lewis SYSTEM \"lewis.dtd\">\n",
      "1: <REUTERS TOPICS=\"YES\" LEWISSPLIT=\"TRAIN\" CGISPLIT=\"TRAINING-SET\" OLDID=\"5544\" NEWID=\"1\">\n",
      "2: <DATE>26-FEB-1987 15:01:01.79</DATE>\n",
      "3: <TOPICS><D>cocoa</D></TOPICS>\n",
      "4: <PLACES><D>el-salvador</D><D>usa</D><D>uruguay</D></PLACES>\n",
      "5: <PEOPLE></PEOPLE>\n",
      "6: <ORGS></ORGS>\n",
      "7: <EXCHANGES></EXCHANGES>\n",
      "8: <COMPANIES></COMPANIES>\n",
      "9: <UNKNOWN> \n",
      "10: &#5;&#5;&#5;C T\n",
      "11: &#22;&#22;&#1;f0704&#31;reute\n",
      "12: u f BC-BAHIA-COCOA-REVIEW   02-26 0105</UNKNOWN>\n",
      "13: <TEXT>&#2;\n",
      "14: <TITLE>BAHIA COCOA REVIEW</TITLE>\n",
      "15: <DATELINE>    SALVADOR, Feb 26 - </DATELINE><BODY>Showers continued throughout the week in\n",
      "16: the Bahia cocoa zone, alleviating the drought since early\n",
      "17: January and improving prospects for the coming temporao,\n",
      "18: although normal humidity levels have not been restored,\n",
      "19: Comissaria Smith said in its weekly review.\n",
      "...\n",
      "32720: </REUTERS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sgmlDocuments = []\n",
    "previewLines = 20\n",
    "\n",
    "filePaths = GetDataFilePaths(\"reuters21578/\")\n",
    "for filePath in filePaths:\n",
    "    sgmlDocuments += ReadDocumentsFromFile(\"reuters21578/reut2-000.sgm\", previewLines)\n",
    "    previewLines = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 22,000 documents from 22 files\n"
     ]
    }
   ],
   "source": [
    "numDocuments = len(sgmlDocuments)\n",
    "print(\"Extracted %s documents from %d files\" % (\"{:,}\".format(numDocuments), len(filePaths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make an index\n",
    "P115: Scoring algorithm\n",
    "  1. \"store N/df_t  at the head of the postings for t\" to compute idf_t\n",
    "  1. \"store the term frequency tf_t,d for each postings entry\"\n",
    "  1. Use a heap as a priority queue for document scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict \n",
    "\n",
    "def ContentTokens(content):\n",
    "    # Replace symbols and numbers with spaces\n",
    "    content = \"\".join([(c if c.isalpha() else \" \") for c in content if c != \"'\"])\n",
    "    allTokens = content.split()\n",
    "    return [x.lower() for x in allTokens if len(x) > 1 and x.replace(\"'\", \"a\").isalpha()]\n",
    "\n",
    "def CombinedTermCounts(a, b):\n",
    "    for term, count in b.items():\n",
    "        a[term] = (a[term] if term in a else 0) + count\n",
    "    return a\n",
    "\n",
    "def GetTermCounts(tagTuple):\n",
    "    # Get term counts for this tag's content\n",
    "    tokens = ContentTokens(tagTuple[TT.CONTENT])\n",
    "    terms = set(tokens)\n",
    "    termCounts = dict([(x, tokens.count(x)) for x in terms])\n",
    "\n",
    "    # Accumulate term counts from sub tags\n",
    "    for _, subTagTuples in tagTuple[TT.SUBTAGS].items():\n",
    "        for subTagTuple in subTagTuples:\n",
    "            subTagTermCounts = GetTermCounts(subTagTuple)\n",
    "            termCounts = CombinedTermCounts(termCounts, subTagTermCounts)\n",
    "\n",
    "    return termCounts\n",
    "\n",
    "class TermPostings:\n",
    "    \"\"\"Postings list with metrics for a single term\"\"\"\n",
    "    def __init__(self):\n",
    "        self.postingsList = dict()\n",
    "        self.cf = 0\n",
    "        self.df = 0\n",
    "        self.idf = 0\n",
    "        self.log_idf = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For each term, count occurrences in all documents\n",
    "postings = dict()\n",
    "documentTermCounts = [dict()] * numDocuments\n",
    "for i, doc in enumerate(sgmlDocuments):\n",
    "    termCounts = GetTermCounts(doc)\n",
    "    documentTermCounts[i] = termCounts\n",
    "\n",
    "    for term, count in termCounts.items():\n",
    "        if term not in postings:\n",
    "            postings[term] = TermPostings()\n",
    "        postings[term].postingsList[i] = (count)\n",
    "\n",
    "# Calculate frequency metrics\n",
    "for _, termPostings in postings.items():\n",
    "    termPostings.cf = np.sum([x for x in index['tin'].postingsList.values()])\n",
    "    termPostings.idf = numDocuments / len(termPostings.postingsList)\n",
    "    termPostings.log_idf = np.log(termPostings.idf)\n",
    "\n",
    "# Sort by term\n",
    "index = OrderedDict([(x, postings[x]) for x in sorted(postings)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build document vectors\n",
    "To make vector scoring possible, we create a normalized vector representation for each document. Indexing and general performance improvements for document vector usage are not covered until chapter 7, so for now we're using brute force methods.\n",
    "\n",
    "Document vectors can be created by normalizing the document term count vectors -- we carry over the (term, count) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightsVector:\n",
    "    \"\"\"Vector representation of a document or query\"\"\"\n",
    "    def __init__(self, termWeights):\n",
    "        self.termWeights = termWeights\n",
    "        self.length = 1\n",
    "\n",
    "    def CosineSimilarity(self, vector):\n",
    "        # It's more efficient if the shorter vector does the calculations\n",
    "        if len(self.termWeights) > len(vector.termWeights):\n",
    "            return vector.CosineSimilarity(self)\n",
    "\n",
    "        # Dot product\n",
    "        dot = np.sum([self.termWeights[x] * vector.termWeights[x] for x in self.termWeights if x in vector.termWeights])\n",
    "        return dot / (self.length * vector.length)\n",
    "\n",
    "    @staticmethod\n",
    "    def FromString(string):\n",
    "        # Extract tokens and count terms\n",
    "        tokens = ContentTokens(string)\n",
    "        terms = set(tokens)\n",
    "        termCounts = dict([(x, tokens.count(x)) for x in terms])\n",
    "        return WeightsVector.FromTermCounts(termCounts)        \n",
    "\n",
    "    @staticmethod\n",
    "    def FromTermCounts(termCounts):\n",
    "        # Normalize term counts\n",
    "        vectorLength = np.sqrt(np.sum([np.square(x) for x in termCounts.values()]))\n",
    "        termWeights = dict([\n",
    "            (termCount[0], termCount[1] / vectorLength)\n",
    "            for termCount in termCounts.items()\n",
    "        ])\n",
    "        return WeightsVector(termWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "documentVectors = [None] * numDocuments\n",
    "for i, counts in enumerate(documentTermCounts):\n",
    "    documentVectors[i] = WeightsVector.FromTermCounts(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run some queries using vector space scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetSimilarVectors(weightsVector):\n",
    "    # Score each document vector\n",
    "    scores = dict()\n",
    "    for i, documentVector in enumerate(documentVectors):\n",
    "        scores[i] = queryVector.CosineSimilarity(documentVector)\n",
    "    \n",
    "    # Sort by highest score\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def GetQueryVector(query):\n",
    "    return WeightsVector.FromString(query)\n",
    "\n",
    "def GetQueryScores(query):\n",
    "    queryVector = GetQueryVector(query)\n",
    "    return GetSimilarVectors(queryVector)\n",
    "\n",
    "def GetBestResult(query):\n",
    "    bestMatch = GetQueryScores(query)[0]\n",
    "    if (bestMatch[1] > 0):\n",
    "        return sgmlDocuments[bestMatch[0]]\n",
    "    else:\n",
    "        print(\"No documents contain any of the queried terms \" + str(ContentTokens(query)))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No documents contain any of the queried terms ['soup', 'kitchen']\n"
     ]
    }
   ],
   "source": [
    "GetBestResult(\"soup kitchen\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "result = GetBestResult(\"Kidder Peabody\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('REUTERS',\n",
       " {'CGISPLIT': '\"TRAINING-SET\"',\n",
       "  'LEWISSPLIT': '\"TRAIN\"',\n",
       "  'NEWID': '\"158\"',\n",
       "  'OLDID': '\"5701\"',\n",
       "  'TOPICS': '\"NO\"'},\n",
       " {'COMPANIES': [('COMPANIES', {}, {}, '')],\n",
       "  'DATE': [('DATE', {}, {}, '26-FEB-1987 17:41:08.82')],\n",
       "  'EXCHANGES': [('EXCHANGES', {}, {}, '')],\n",
       "  'ORGS': [('ORGS', {}, {}, '')],\n",
       "  'PEOPLE': [('PEOPLE', {}, {}, '')],\n",
       "  'PLACES': [('PLACES', {}, {'D': [('D', {}, {}, 'usa')]}, '')],\n",
       "  'TEXT': [('TEXT',\n",
       "    {},\n",
       "    {'BODY': [('BODY',\n",
       "       {},\n",
       "       {},\n",
       "       'Presidential Airways Inc said its joint marketing and services agreement with Texas Air Corp\\'s &lt;TXN> Continental Airlines unit was approved by the U.S. Department of Justice. According to the agreement, Presidential Airways will operate scheduled service under the name \"Continental Express.\" The company, however, will remain independent. Reuter &#3;')],\n",
       "     'DATELINE': [('DATELINE', {}, {}, 'WASHINGTON, FEB 26 -')],\n",
       "     'TITLE': [('TITLE',\n",
       "       {},\n",
       "       {},\n",
       "       'PRESIDENTIAL AIRWAYS &lt;PAIR> PACT APPROVED')]},\n",
       "    '&#2;')],\n",
       "  'TOPICS': [('TOPICS', {}, {}, '')],\n",
       "  'UNKNOWN': [('UNKNOWN',\n",
       "    {},\n",
       "    {},\n",
       "    '&#5;&#5;&#5;F &#22;&#22;&#1;f0220&#31;reute d f BC-PRESIDENTIAL-AIRWAYS 02-26 0069')]},\n",
       " '')"
      ]
     },
     "execution_count": 582,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GetBestResult(\"presidential election\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
